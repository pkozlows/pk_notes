\section{Formulation for the dTDA}
\subsection{Comparing to Booth's ED}
For simplicity, we will just work with a single candle, the lesser one. In the dTDA case, Booth's formulation for the upfolded Hamiltonian is
\begin{equation}
    \bm{H} = \begin{pmatrix} \bm{F} & \bm{W} \\ \bm{W}^{\dagger} & \bm{d} \end{pmatrix}
\label{eq:booth_hamiltonian}
\end{equation}
where we have the definitions
\begin{equation}
    W_{pkv} = \sum_{ia} (pk|ia) X_{ia}^{v} \quad \text{and} \quad d_{kv,lv'} = \left(\epsilon_k - \Omega_v\right) \delta_{k,l} \delta_{v,v'}
\label{eq:booth_definitions}
\end{equation}
Now, Tim's version of the Hamiltonian is given by
\begin{equation}
    \bm{H} = \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}}\\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \bm{C}^{2 \mathrm{hlp}} \end{pmatrix}
\label{eq:tim_hamiltonian}
\end{equation}
where the definitions of the matrix elements are
\begin{align}
    V_{p, k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} &= \langle p c | k l \rangle \equiv (pk|lc) \\
    C_{i[j a], k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} &= \left[\left(\epsilon_i+\epsilon_j-\epsilon_a\right) \delta_{j l} \delta_{a c}-\langle j c | a l \rangle\right] \delta_{i k}
\end{align}
and in particular, we have a definition 
\begin{equation}
    \bm{C}^{2 \mathrm{hlp}} = \epsilon^{1 \mathrm{~h}} \oplus (-\bm{A}) = \epsilon^{1 \mathrm{~h}} \otimes \bm{1} + \bm{1} \otimes (-\bm{A})
\label{eq:tim_c_hlp}
\end{equation}
Let us define a unitary rotation $\bm{U} = \bm{1} \oplus_{\text{diag}} \bm{X}$. Application of this unitary to the Hamiltonian will not change the spectrum and actually transforms the problem into
\begin{equation}
    \bm{H}' = \bm{U}^\dag \bm{H} \bm{U} = \begin{pmatrix} \bm{1} & \bm{0}\\ \bm{0}&\bm{X }^\dag \end{pmatrix} \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}}\\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \bm{C}^{2 \mathrm{hlp}} \end{pmatrix} \begin{pmatrix} \bm{1} & \bm{0} \\ \bm{0}&\bm{X } \end{pmatrix} = \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}}\bm{X}\\ \bm{X}^\dag \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \bm{X }^\dagger \bm{C}^{2 \mathrm{hlp}} \bm{X } \end{pmatrix}.
\label{eq:booth_upfolded_hamiltonian}
\end{equation}
Now let us evaluate $\bm{X}^\dagger \bm{C}^{2 \mathrm{hlp}} \bm{X }$ in the dTDA case. We have
\begin{align}
    \bm{X}^\dagger \bm{C}^{2 \mathrm{hlp}} \bm{X } &= \bm{X}^\dagger \left(\epsilon^{1 \mathrm{~h}} \otimes \bm{1} + \bm{1} \otimes (-\bm{A})\right) \bm{X } \\
&= \bm{X}^\dagger \left(\epsilon^{1 \mathrm{~h}} \otimes \bm{1}\right) \bm{X } + \bm{X}^\dagger \left(\bm{1} \otimes (-\bm{A})\right) \bm{X } \\
&= \left(\epsilon^{1 \mathrm{~h}} \underbrace{\bm{X}^\dag \bm{X}}_{\bm{I}} \right) \otimes \left(\underbrace{\bm{X}^\dag \bm{X}}_{\bm{I}}\right) + \left(\underbrace{\bm{X}^\dag \bm{X}}_{\bm{I}}\right) \otimes (-\underbrace{\bm{X}^\dag \bm{A} \bm{X}}_{\bm{\Omega }}) \\
&= \epsilon^{1 \mathrm{~h}} \oplus (-\bm{\Omega }) \equiv \bm{d}^{<}
\end{align}
where we have used the fact that $\bm{X}^\dag \bm{X} = \bm{1}$, since $\bm{X}$ is unitary. Similarly, we can evaluate the other term
\begin{align}
    \bm{V}^{2 \mathrm{~h} 1 \mathrm{p}} \bm{X } &= \bm{W}^< \equiv \sum_{lc} (pk|lc) X_{lc}^{v} \\
\end{align}
where we have used the definition of the $W$ matrix in Booth's formulation, as given in \eqref{eq:booth_definitions}.
\subsection{Deriving the matrix vector products}
Now, we can define a vector $\bm{R} = ( r_i,\; r_a,\; r_{i[jb]},\; r_{[jb]a} )$. Application of the Hamiltonian to this vector gives us the matrix-vector product $\bm{H} \bm{R} = \bm{\sigma }$, where $\bm{\sigma} = ( \sigma_i,\; \sigma_a,\; \sigma_{i[jb]},\; \sigma_{[jb]a} )$. Consider
\begin{align}
\bm{H} \bm{R} &= \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}} & \bm{V}^{2 \mathrm{p} 1 \mathrm{~h}} \\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \bm{C}^{2 \mathrm{hlp}} & \bm{0} \\ \left(\bm{V}^{2 \mathrm{plh}}\right)^{\dagger} & \bm{0} & \bm{C}^{2 \mathrm{plh}} \end{pmatrix} \begin{pmatrix} r_i \\ r_a \\ r_{i[jb]} \\ r_{[jb]a} \end{pmatrix} = \begin{pmatrix} \sigma_i \\ \sigma_a \\ \sigma_{i[jb]} \\ \sigma_{[jb]a} \end{pmatrix} \\[6pt]
\end{align}
Let us enumerate now what we actually will get:
\begin{align}
\sigma_i &= 
  \sum_{j} f_{i j}\,r_j
  + \sum_{b} f_{i b}\,r_b
  + \sum_{k l c} \bigl\langle i\,c | k\,l \bigr\rangle\,r_{k[l c]}
  + \sum_{k c d} \bigl\langle i\,k | d\,c \bigr\rangle\,r_{[k c]d}, \\[6pt]
\sigma_a &=
    \sum_{j} f_{a j}\,r_j
    + \sum_{b} f_{a b}\,r_b
    + \sum_{k l c} \bigl\langle a\,c | k\,l \bigr\rangle\,r_{k[l c]}
    + \sum_{k c d} \bigl\langle a\,k | d\,c \bigr\rangle\,r_{[k c]d}, \\[6pt]
\sigma_{i[ja]} &
    \sum_{k} \bigl\langle k\,a | i\,j \bigr\rangle\,r_k
    + \sum_{b} \bigl\langle b\,a | i\,j \bigr\rangle\,r_b
    + \left(\epsilon _{i} + \epsilon _{j} - \epsilon _{a}\right) r_{i[j a]}
    - \sum_{lc} \bigl\langle j\,c | a\,l \bigr\rangle\,r_{i[l c]} \\[6pt]
\sigma_{[ia]b} &= 
    \sum_{j} \bigl\langle j\,i | b\,a \bigr\rangle\,r_j
    + \sum_{c} \bigl\langle c\,i | b\,a \bigr\rangle\,r_c
    + \left(\epsilon _{a} + \epsilon _{b} - \epsilon _{i}\right) r_{[i a] b}
    + \sum_{k c} \bigl\langle a\,k | i\,c \bigr\rangle\,r_{[k c] b}.
\end{align}
\section{Formulation for the dRPA}
\subsection{Deriving the dRPA approach as they propose}
\subsubsection{Showing equivalence between excitation energies of M and Mtilde}
So we start with this generalized eigenvalue equation
$$
\mathbf{M}\bm{Z}=\mathbf{N}\bm{Z}\left(\begin{array}{cc}
\boldsymbol{\Omega}_{+} & 0 \\
0 & -\boldsymbol{\Omega}_{+}
\end{array}\right)
$$
where
$$
\mathbf{M} =\left(\begin{array}{ll}
\mathbf{A} & \mathbf{B} \\
\mathbf{B} & \mathbf{A}
\end{array}\right) \quad 
\mathbf{N} =\left(\begin{array}{cc}
\mathbf{1} & \mathbf{0} \\
\mathbf{0} & -\mathbf{1}
\end{array}\right) \quad
\bm{Z} =\left(\begin{array}{ll}
\bm{X} & \bm{Y} \\
\bm{Y} & \bm{X}
\end{array}\right)
$$
and $\boldsymbol{\Omega}_{+}$is a diagonal matrix of positive excitation energies. Left multiplying both sides by $\bm{N}$ and right multiplying by $\bm{Z}^{-1}$ gives us
\begin{equation}
    \bm{N}\bm{M}\underbrace{\bm{Z} \bm{Z}^{-1}}_{\bm{1}} = \underbrace{\bm{N}\bm{N}}_{\bm{1}}\bm{Z}\left(\begin{array}{cc}
\boldsymbol{\Omega}_{+} & 0 \\
0 & -\boldsymbol{\Omega}_{+}
\end{array}\right) \bm{Z^{-1}} \implies - \bm{N}\bm{M} = -\bm{Z}\left(\begin{array}{cc}\boldsymbol{\Omega}_{+} & 0 \\
0 & -\boldsymbol{\Omega}_{+}
\end{array}\right)\bm{Z}^{-1}
\end{equation}
Now we can use the fact that the action of a scalar function $f$, such as the step function, on a diagonalizable matrix $\bm{X} \equiv \bm{Y} \bm{\Lambda} \bm{Y}^{-1}$ can be expressed as
$$
f(\bm{X}) = \bm{Y} f(\bm{\Lambda}) \bm{Y}^{-1}
$$
so we can write
\begin{equation}
    \Theta(-\bm{N}\bm{M}) = \bm{Z} \left(\begin{array}{cc}\Theta(-\boldsymbol{\Omega}_{+}) & 0 \\ 0 & \Theta(\boldsymbol{\Omega}_{+})\end{array}\right)\bm{Z}^{-1} = \bm{Z} \left(\begin{array}{cc}\bm{0} & 0 \\ 0 & \bm{1}\end{array}\right)\bm{Z}^{-1}
\end{equation}
and so it becomes clear that if we define $\tilde{\bm{M}} = \bm{M} + \eta \bm{N}\Theta(-\bm{N}\bm{M})$, we can write
\begin{equation}
    \tilde{\bm{M}} \bm{Z} = \bm{M} \bm{Z} + \eta \bm{N}\Theta(-\bm{N}\bm{M}) \bm{Z} = \mathbf{N}\bm{Z}\left(\begin{array}{cc}
\boldsymbol{\Omega}_{+} & 0 \\
0 & -\boldsymbol{\Omega}_{+}
\end{array}\right) + \bm{N}\bm{Z} \left(\begin{array}{cc}\bm{0} & 0 \\ 0 & \bm{\eta}\end{array}\right) \underbrace{\bm{Z}^{-1} \bm{Z}}_{ \bm{1}} = \bm{N}\bm{Z}\left(\begin{array}{cc}\boldsymbol{\Omega}_{+} & 0 \\ 0 & -\boldsymbol{\Omega}_{+} + \bm{\eta}\end{array}\right).
\end{equation}
\subsubsection{Downfolding equivalence}
Now they define a super-matrix,
\begin{equation}
\bm{H} =
\begin{pmatrix}
\bm{F} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{plh}} & \bm{V}^{2\mathrm{plh}} \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  &  & & \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  & \bm{C}^{2\mathrm{hlp}} & & \bm{0} \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} & & & & \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} &  & \bm{0} & & \bm{C}^{2\mathrm{plh}}
\end{pmatrix}
\end{equation}
where $\mathbf{C}^{2 \mathrm{hlp}}=\varepsilon^{1 \mathrm{~h}} \oplus(-\tilde{\mathbf{M}})$ and $\mathbf{C}^{2 \mathrm{plh}}=\varepsilon^{1 \mathrm{p}} \oplus \tilde{\mathbf{M}}$, which for which we are instructed to use the super-metric
$$
\bm{\mathcal{N}}=\left(\begin{array}{ccc}
\bm{1} & 0 & 0 \\
0 & \bm{1} \oplus \bm{N} & 0 \\
0 & 0 & \bm{1} \oplus \bm{N}
\end{array}\right)
$$
and the excitation vector is $\bm{R} = (\bm{R}^{1\mathrm{h}+1\mathrm{p}}, \bm{R}^{2\mathrm{h}1\mathrm{p}}, \bm{R}^{2\mathrm{p}1\mathrm{h}})^\dag$ with $\bm{R}^{2\mathrm{h}1\mathrm{p}}=(\bm{r}^{2\mathrm{h}1\mathrm{p}}, \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}})^\dag$ and $\bm{R}^{2\mathrm{p}1\mathrm{h}}=(\bm{r}^{2\mathrm{p}1\mathrm{h}}, \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}})^\dag$.
% \begin{tcolorbox}[colback=red!10!white, colframe=red!50!black, title=Suspicion for their motivation in constructing a supermatrix of this form]

% We know that the analytic form for the correlation self energy is
% \begin{equation}
%     \Sigma_{pq}^{\text{corr}}(\omega) = \sum_{\mu }^{\text{RPA}}\left(\sum_{i}^{\text{occupied}} \frac{w_{pi}^{\mu }w_{iq}^{\mu }}{\omega -(\epsilon _{i}-\Omega  _{\mu })+i\eta}+ \sum_{a}^{\text{virtual}} \frac{w_{pa}^{\mu }w_{aq}^{\mu }}{\omega -(\epsilon _{a}+\Omega  _{\mu })-i\eta}\right)
% \label{eq:correlation_self_energy}
% \end{equation}
% with
% \begin{equation}
%     w_{pq}^{\mu} = \sum_{jb} (pq|jb) \left(X_{jb}^{\mu} + Y_{jb}^{\mu}\right)
% \end{equation}
% and if we consider the downfolding of $\bm{H}$, I suspect that they want to achieve a form 
% \begin{align}
%     \Sigma_{pq}^{\text{corr}}(\omega) &= \sum_{\mu }^{\text{RPA}}\left(\sum_{i}^{\text{occupied}} \frac{w_{pi}^{\mu }w_{iq}^{\mu }}{\omega -(\epsilon _{i}-\Omega  _{\mu })+i\eta}+ \sum_{a}^{\text{virtual}} \frac{w_{pa}^{\mu }w_{aq}^{\mu }}{\omega -(\epsilon _{a}+\Omega  _{\mu })-i\eta}\right)\\
% & + \sum_{\nu(>>\mu) }^{\text{shifted}}\left(\sum_{i}^{\text{occupied}} \frac{w_{pi}^{\nu }w_{iq}^{\nu }}{\omega -(\epsilon _{i}-\Omega  _{\nu })+i\eta}+ \sum_{a}^{\text{virtual}} \frac{w_{pa}^{\nu }w_{aq}^{\nu }}{\omega -(\epsilon _{a}+\Omega  _{\nu })-i\eta}\right)
% \end{align}
% But remember that by design the $\Omega _{\nu}$ are  shifted to large positive values and so therefore the second term can be neglected and we can recover \ref{eq:correlation_self_energy}. So probably the downfolding is trying to achieve the qualitative form
% \begin{align}
%     \bm{\Sigma }^c(\omega) &= \left(\frac{ \bm{V}^{2\mathrm{h1p}} \left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger}}{\omega \bm{I}-\bm{C}^{2\mathrm{hlp}}} + \frac{ \bm{V}^{2\mathrm{p1h}} \left(\bm{V}^{2\mathrm{p1h}}\right)^{\dagger}}{\omega \bm{I}-\bm{C}^{2\mathrm{plh}}}\right) \oplus \left(\frac{ \bm{V}^{2\mathrm{h1p}} \left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger}}{\omega \bm{I}-\bm{C}^{2\mathrm{hlp}}} + \frac{ \bm{V}^{2\mathrm{p1h}} \left(\bm{V}^{2\mathrm{p1h}}\right)^{\dagger}}{\omega \bm{I}-\bm{C}^{2\mathrm{plh}}}\right)
% \end{align}
% but how can this be done if we know that $\bm{C}^{2\mathrm{hlp}}=\epsilon^{1\mathrm{~h}} \oplus (-\tilde{\bm{M}})$ and from above we know that $\bm{N}\tilde{\bm{M}} = \bm{Z}\left(\begin{array}{cc}\bm{\Omega } &\bm{0}  \\ \bm{0} & \bm{\zeta}-\bm{\Omega }\end{array}\right)\bm{Z}^{-1}$, where $\zeta$ is a large positive number.
% \end{tcolorbox}
\subsubsection{Proving ED equivalence}
We have the eigenproblem $\bm{\mathcal{N}} \bm{H} \bm{R} = \bm{R} \bm{E}$, so we really care about the matrix-vector products $\bm{\sigma }=\mathcal{\bm{N}}\bm{H}\bm{R}$. The answer they give is:
\begin{align}
\sigma_i &= \sum_{j} f_{i j}\,r_j + \sum_{b} f_{i b}\,r_b + \sum_{k l c} \bigl\langle i\,c | k\,l \bigr\rangle\,r_{k[l c]} + \sum_{k c d} \bigl\langle i\,k | d\,c \bigr\rangle\,r_{[k c]d} + \sum_{k l c} \bigl\langle i\,c | k\,l \bigr\rangle\,\bar{r}_{k[l c]} + \sum_{k c d} \bigl\langle i\,k | d\,c \bigr\rangle\,\bar{r}_{[k c]d} \\
\sigma_a &= \sum_{j} f_{a j}\,r_j + \sum_{b} f_{a b}\,r_b + \sum_{k l c} \bigl\langle a\,c | k\,l \bigr\rangle\,r_{k[l c]} + \sum_{k c d} \bigl\langle a\,k | d\,c \bigr\rangle\,r_{[k c]d} + \sum_{k l c} \bigl\langle a\,c | k\,l \bigr\rangle\,\bar{r}_{k[l c]} + \sum_{k c d} \bigl\langle a\,k | d\,c \bigr\rangle\,\bar{r}_{[k c]d} \\
    \sigma _{i[ja]} &= \sum_{k} \bigl\langle k\,a | i\,j \bigr\rangle\,r_k + \sum_{b} \bigl\langle b\,a | i\,j \bigr\rangle\,r_b + \varepsilon_i r_{i[j a]} - \sum_{k b} \bigl[\mathbf{N} \tilde{\mathbf{M}}\bigr]_{j a k b}^{\mathrm{xx}} r_{i[k b]} - \sum_{k b} \bigl[\mathbf{N} \tilde{\mathbf{M}}\bigr]_{j a k b}^{\mathrm{xd}} \bar{r}_{i[k b]} \\
    \bar{\sigma}_{i[ja]} &= -\sum_{k} \bigl\langle k\,a | i\,j \bigr\rangle\,r_k - \sum_{b} \bigl\langle b\,a | i\,j \bigr\rangle\,r_b + \varepsilon_i \bar{r}_{i[j a]} - \sum_{k b} \bigl[\mathbf{N} \tilde{\mathbf{M}}\bigr]_{j a, k b}^{\mathrm{dx}} r_{i[k b]} - \sum_{k b} \bigl[\mathbf{N} \tilde{\mathbf{M}}\bigr]_{j a, k b}^{\mathrm{dd}} \bar{r}_{i[k b]} \\
\sigma _{[ia]b} &= \sum_{j} \bigl\langle j\,i | b\,a \bigr\rangle\,r_j + \sum_{c} \bigl\langle c\,i | b\,a \bigr\rangle\,r_c + \varepsilon_b r_{[i a] b} + \sum_{jc}\bigl[ \mathbf{N} \tilde{\mathbf{M}}\bigr]_{i a, j c}^{\mathrm{xx}} r_{[j c] b} + \sum_{jc}\bigl[ \mathbf{N} \tilde{\mathbf{M}}\bigr]_{i a, j c}^{\mathrm{xd}} \bar{r}_{[j c] b} \\
\bar{\sigma}_{[ia]b} &= -\sum_{j} \bigl\langle j\,i | b\,a \bigr\rangle\,r_j - \sum_{c} \bigl\langle c\,i | b\,a \bigr\rangle\,r_c + \varepsilon_b \bar{r}_{[i a] b} + \sum_{jc}\bigl[ \mathbf{N} \tilde{\mathbf{M}}\bigr]_{i a, j c}^{\mathrm{dx}} r_{[j c] b} + \sum_{jc}\bigl[ \mathbf{N} \tilde{\mathbf{M}}\bigr]_{i a, j c}^{\mathrm{dd}} \bar{r}_{[j c] b}
\end{align}
% $$
% \begin{aligned}
% \sigma_i= & \sum_j f_{i j} r_j+\sum_b f_{i b} r_b+\sum_{k l c}\langle i c | k l\rangle r_{k[l c]}+\sum_{k c d}\langle i k | d c\rangle r_{[k c] d} \\
% & +\sum_{k l c}\langle i c | k l\rangle \bar{r}_{k[l c]}+\sum_{k c d}\langle i k | d c\rangle \bar{r}_{[k c] d} \\
% \sigma_a= & \sum_j f_{a j} r_j+\sum_b f_{a b} r_b+\sum_{k l c}\langle a c | k l\rangle r_{k[l c]}+\sum_{k c d}\langle a k | d c\rangle r_{[k c] d} \\
% & +\sum_{k l c}\langle a c | k l\rangle \bar{r}_{k[l c]}+\sum_{k c d}\langle a k | d c\rangle \bar{r}_{[k c] d}
% \end{aligned}
% $$
\begin{tcolorbox}[colback=red!10!white, colframe=red!50!black, title=It is unclear where these equations come from]
I found that these equations give rise to nearly the same spectrum (test on LiH with cc-pvdz basis show up to $~10^{-9} Ha$ of agreement) as that of the exact upfolded $GW$ Hamiltonian (Booth's ED), which is given by
\begin{equation}
    \bm{H}_{\text{Upfolded}}^{G_0W_0} = \begin{pmatrix} \bm{F} & \bm{W}^< & \bm{W}^> \\ \bm{W}^{<,\dagger} & \bm{d}^< & 0 \\ \bm{W}^{>, \dagger} & 0 & \bm{d}^> \end{pmatrix}
\label{eq:booth_hamiltonian}
\end{equation}
where we have the definitions
\begin{align}
    W_{pkv}^< > = \left(\epsilon_k + \Omega_v\right) \delta_{k,l} \delta_{v,v'}
\label{eq:booth_definitions}
\end{align}
It concerns me that the agreement isn't exact (below $10^{-13} Ha$). I would want to have this cleared up before I try to reduce the scaling by implementing the Arnoldi procedure they try. Also, I am not able to figure out what motivated the form for this upfolded Hamiltonian for Tim's GW-RPA. Specifically, if one interprets the $\oplus$ as a direct sum, then the shapes of the matrices are not compatible since, for example, $\mathbf{C}^{2 \mathrm{hlp}}=\varepsilon^{1 \mathrm{~h}} \oplus(-\tilde{\mathbf{M}})$ has the column dimension $O+2OV$ which does not equal the width of its upstairs neighbor $\begin{pmatrix}
    \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{h1p}}
\end{pmatrix}$, which is $2O^2V$. If one interprets the $\oplus$ as a Kronecker sum, the shapes are compatible, but the energies are wrong. If we assume that the $\oplus$ should actually be the Kronecker product $\otimes$, again the shapes will be compatible, but the energies are wrong. I asked Jinghong, Nemo, and Hamlin and no one was able to figure it out. Can I email Tim to ask him about this?
%  that means I am not able to exactly identify the proper preconditioner, which is usually the diagonal of the matrix. But if the preconditioner just has the function of accelerating convergence, and does not determine the final results, then I am not sure if this is a problem.
%  I don't want to implement the Arnoldi iteration that they suggest to obtain lower scaling until I know understand the discrepancy for this high scaling version.
\end{tcolorbox}

To figure out how to recreate them consider the matrix multiplication
\begin{align}
&\bm{\mathcal{N}}^{2\mathrm{h1p}} \bm{H}^{2\mathrm{h1p}} \bm{R}^{2\mathrm{h1p}} \\
&= \begin{pmatrix}
\bm{1}_{P,P} & 0 \\
0 & \bm{X} \\
\end{pmatrix}
\begin{pmatrix}
\bm{F}_{P,P} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{h1p}} \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  &  \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  & \bm{C}^{2\mathrm{hlp}}
\end{pmatrix}
\begin{pmatrix}
r_i \\
r_a \\
r_{i[j b]} \\
\bar{r}_{i[j b]} \\
\end{pmatrix}\\
&= \begin{pmatrix}
\bm{1}_{P,P} & 0 \\
0 & \bm{X} \\
\end{pmatrix}
\begin{pmatrix}
\bm{f}_{O,O}r^i_{O,\bar{A}} + \bm{f}_{O,V}r^a_{V,\bar{A}} + \bm{v}^{2\mathrm{h1p}}_{O,O^2V} r^{i[j b]}_{O^2V, \bar{A}} + \bm{v}^{2\mathrm{h1p}}_{O,O^2V} \bar{r}^{i[j b]}_{O^2V, \bar{A}} \\
\bm{f}_{V,O}r^i_{O,\bar{A}} + \bm{f}_{V,V}r^a_{V,\bar{A}} + \bm{v}^{2\mathrm{h1p}}_{V,O^2V} r^{i[j b]}_{O^2V, \bar{A}} + \bm{v}^{2\mathrm{h1p}}_{V,O^2V} \bar{r}^{i[j b]}_{O^2V, \bar{A}} \\
\bm{v}^{2\mathrm{h1p},\dagger}_{O^2V,O} r^i_{O,\bar{A}} + \bm{v}^{2\mathrm{h1p},\dagger}_{O^2V,V} r^a_{V,\bar{A}} + \bm{c}^{2\mathrm{hlp},xx}_{O^2V,O^2V} r^{i[j b]}_{O^2V, \bar{A}} + \bm{c}^{2\mathrm{hlp},xd}_{O^2V,O^2V} \bar{r}^{i[j b]}_{O^2V, \bar{A}} \\
\bm{v}^{2\mathrm{h1p},\dagger}_{O^2V,O} r^i_{O,\bar{A}} + \bm{v}^{2\mathrm{h1p},\dagger}_{O^2V,V} r^a_{V,\bar{A}} + \bm{c}^{2\mathrm{hlp},dx}_{O^2V,O^2V} r^{i[j b]}_{O^2V, \bar{A}} + \bm{c}^{2\mathrm{hlp},dd}_{O^2V,O^2V} \bar{r}^{i[j b]}_{O^2V, \bar{A}} \\
\end{pmatrix}
\\
&= \begin{pmatrix}
\sigma_i \\
\sigma_a \\
\sigma_{i[j a]} \\
\bar{\sigma}_{i[j a]} \\
\end{pmatrix}
=\begin{pmatrix}
\sum_{j} f_{i j}\,r_j + \sum_{b} f_{i b}\,r_b + \sum_{k l c} \bigl\langle i\,c | k\,l \bigr\rangle\,r_{k[l c]}+ \sum_{k l c} \bigl\langle i\,c | k\,l \bigr\rangle\,\bar{r}_{k[l c]}  \\
\sum_{j} f_{a j}\,r_j + \sum_{b} f_{a b}\,r_b + \sum_{k l c} \bigl\langle a\,c | k\,l \bigr\rangle\,r_{k[l c]} + \sum_{k l c} \bigl\langle a\,c | k\,l \bigr\rangle\,\bar{r}_{k[l c]} \\
\sum_{k} \bigl\langle k\,a | i\,j \bigr\rangle\,r_k + \sum_{b} \bigl\langle b\,a | i\,j \bigr\rangle\,r_b + \varepsilon_i r_{i[j a]} - \sum_{k b} \bigl[\mathbf{N} \tilde{\mathbf{M}}\bigr]_{j a k b}^{\mathrm{xx}} r_{i[k b]} - \sum_{k b} \bigl[\mathbf{N} \tilde{\mathbf{M}}\bigr]_{j a k b}^{\mathrm{xd}} \bar{r}_{i[k b]} \\
-\sum_{k} \bigl\langle k\,a | i\,j \bigr\rangle\,r_k - \sum_{b} \bigl\langle b\,a | i\,j \bigr\rangle\,r_b + \varepsilon_i \bar{r}_{i[j a]} - \sum_{k b} \bigl[\mathbf{N} \tilde{\mathbf{M}}\bigr]_{j a, k b}^{\mathrm{dx}} r_{i[k b]} - \sum_{k b} \bigl[\mathbf{N} \tilde{\mathbf{M}}\bigr]_{j a, k b}^{\mathrm{dd}} \bar{r}_{i[k b]} \\
\end{pmatrix}
\end{align}
They say that $\bm{V}^{2\mathrm{h1p}}$ has the  elements $V_{p, k[ia]}^{2 \mathrm{~h} 1 \mathrm{p}} = \langle p a | k i \rangle \equiv (pk|ia)$ so we say that $\bm{v}^{2\mathrm{h1p}}_{O,O^2V}$ is the first O rows of $\bm{V}^{2\mathrm{h1p}}$, while $\bm{v}^{2\mathrm{h1p}}_{V,O^2V}$ is the latter V rows of $\bm{V}^{2\mathrm{h1p}}$.
$\bm{C}^{2\mathrm{hlp}}$ is defined as $\varepsilon^{1h} \oplus -\tilde{\bm{M}}.$ They also say that $\bm{X}=1\oplus \bm{N}$, where $\bm{N}=\begin{pmatrix} \bm{1}_{OV} & 0 \\ 0 & -\bm{1}_{OV} \end{pmatrix}$, but I am doubting this because the shapes in the matrix multiplication don't work out then.
Define $\begin{pmatrix} \bm{1}_{OV} & 0 \\
0 & -\bm{1}_{OV} \end{pmatrix} = \bm{N}$ and we have $V_{p, k[ia]}^{2 \mathrm{~h} 1 \mathrm{p}} = \langle p a | k i \rangle \equiv (pk|ia)$. To simplify matters, let's just consider the $2\mathrm{h1p}$ sector. Writing this down gives
\begin{align}
&\bm{\mathcal{N}} \bm{H}^{2\mathrm{h1p}}\\ &= \begin{pmatrix}
\bm{1}_{P,P} & 0 \\
0 & \bm{1}_{O,O} \oplus \bm{N}_{2OV,2OV} \\
\end{pmatrix}
\begin{pmatrix}
\bm{F} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{h1p}} \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  &  \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  & \bm{C}^{2\mathrm{hlp}}
\end{pmatrix} \\
&= \begin{pmatrix}\bm{F}_{P,P} & \bm{1}_{P,P} \bm{V}^{2\mathrm{h1p}}_{P,2O^2V} & \bm{1}_{P,P} \bm{V}^{2\mathrm{h1p}}_{P,2O^2V} \\
& &  &  & & \\
\begin{pmatrix}
\bm{1}_{O,O} & 0 \\
0 & \bm{N}_{2OV,2OV}
\end{pmatrix}
\begin{bmatrix}
\left( \bm{V}^{2\mathrm{h1p}} \right)^{\dagger} \\ \left( \bm{V}^{2\mathrm{h1p}}\right)^{\dagger}\end{bmatrix}_{2O^2V,P} &  &\begin{pmatrix}
\bm{1}_{O,O} & 0 \\
0 & \bm{N}_{2OV,2OV}
\end{pmatrix}\begin{pmatrix}
    \epsilon^{1\mathrm{h}}_{O,O} & 0 \\
    0 & -\tilde{\mathbf{M}}_{2OV,2OV}
\end{pmatrix} \\
\end{pmatrix}\\
&= \begin{pmatrix}\bm{F}_{P,P} & \bm{1}_{P,P} \bm{V}^{2\mathrm{h1p}}_{P,2O^2V} & \bm{1}_{P,P} \bm{V}^{2\mathrm{h1p}}_{P,2O^2V} \\
& &  &  & & \\
\begin{pmatrix}
\bm{1}_{O,O} & 0 \\
0 & \bm{N}_{2OV,2OV}
\end{pmatrix}
\begin{bmatrix}
\left( \bm{V}^{2\mathrm{h1p}} \right)^{\dagger} \\ \left( \bm{V}^{2\mathrm{h1p}}\right)^{\dagger}\end{bmatrix}_{2O^2V,P} &  &\begin{pmatrix}
    \epsilon^{1\mathrm{h}}_{O,O} & 0 \\
    0 & -\mathbf{N}\tilde{\bm{M}}_{2OV,2OV}
\end{pmatrix} \\
\end{pmatrix}
\end{align}
% \begin{align}
% &\bm{\mathcal{N}} \bm{H}\\ &= \begin{pmatrix}
% \bm{1}_{P,P} & 0 & 0 \\
% 0 & \bm{1}_{O,O} \oplus \bm{N}_{2OV,2OV} & 0 \\
% 0 & 0 & \bm{1}_{V,V} \oplus \bm{N}_{2OV,2OV}
% \end{pmatrix}
% \begin{pmatrix}
% \bm{F} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{plh}} & \bm{V}^{2\mathrm{plh}} \\
% \left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  &  & & \\
% \left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  & \bm{C}^{2\mathrm{hlp}} & & \bm{0} \\
% \left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} & & & & \\
% \left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} &  & \bm{0} & & \bm{C}^{2\mathrm{plh}}
% \end{pmatrix} \\
% &= \begin{pmatrix}\bm{F}_{P,P} & \bm{1}_{P,P} \bm{V}^{2\mathrm{h1p}}_{P,A} & \bm{1}_{P,P} \bm{V}^{2\mathrm{h1p}}_{P,A} & \bm{1}_{P,P} \bm{V}^{2\mathrm{plh}}_{P,\bar{A}} & \bm{1}_{P,P} \bm{V}^{2\mathrm{plh}}_{P,\bar{A}} \\
% & &  &  & & \\
% \left[ \bm{1} \oplus \bm{N} \right]_{2A,2A}\begin{bmatrix}
% \left( \bm{V}^{2\mathrm{h1p}} \right)^{\dagger} \\ \left( \bm{V}^{2\mathrm{h1p}}\right)^{\dagger}\end{bmatrix}_{2A,P} &  & \left[ \bm{1} \oplus \bm{N} \right]_{2A,2A}\bm{C}^{2\mathrm{hlp}}_{2A,2A} & & \bm{0}\\
% & & & & & \\
% \left[ \bm{1} \oplus \bm{N} \right]_{2\bar{A},2\bar{A}}\begin{bmatrix}
% \left( \bm{V}^{2\mathrm{plh}} \right)^{\dagger} \\ \left( \bm{V}^{2\mathrm{plh}}\right)^{\dagger}\end{bmatrix}_{2\bar{A},P} &  & \bm{0} & & \left[ \bm{1} \oplus \bm{N} \right]_{2\bar{A},2\bar{A}}\bm{C}^{2\mathrm{plh}}_{2\bar{A},2\bar{A}}
% \end{pmatrix}
% \end{align}
 If we assume that the virtual space is largest, this means that the column width of the excitation vector is $\bar{A}$. The action of the super-metric on the excitation vector is given by
$\bm{\mathcal{N}} \bm{H} \bm{R} = \bm{R} \bm{E}$ leads to:
\begin{equation}
\begin{pmatrix}
\bm{1} & 0 & 0 \\
0 & \bm{1} \oplus \bm{N} & 0 \\
0 & 0 & \bm{1} \oplus \bm{N}
\end{pmatrix}
\begin{pmatrix}
\bm{F} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{plh}} & \bm{V}^{2\mathrm{plh}} \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  &  & & \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  & \bm{C}^{2\mathrm{hlp}} & & \bm{0} \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} & & & & \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} &  & \bm{0} & & \bm{C}^{2\mathrm{plh}}
\end{pmatrix}_{T,T}
\begin{pmatrix}
\bm{r}^{1\mathrm{h}+1\mathrm{p}} \\
\bm{r}^{2\mathrm{h}1\mathrm{p}} \\
\bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} \\
\bm{r}^{2\mathrm{p}1\mathrm{h}} \\
\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}
\end{pmatrix}_{T,\bar{A}} = E
\begin{pmatrix}
\bm{r}^{1\mathrm{h}+1\mathrm{p}} \\
\bm{r}^{2\mathrm{h}1\mathrm{p}} \\
\bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} \\
\bm{r}^{2\mathrm{p}1\mathrm{h}} \\
\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}
\end{pmatrix}_{T,\bar{A}}
\end{equation}

From this we get:
\begin{align}
&
\begin{pmatrix}
\bm{1}_{P,P} & 0 & 0 \\
0 & \left(\bm{1}_{O} \oplus_{\text{kron}} \bm{N}_{2OV}\right)_{2A,2A} & 0 \\
0 & 0 & \left(\bm{1}_V \oplus_{\text{kron}} \bm{N}_{2OV}\right)_{2\bar{A},2\bar{A}}
\end{pmatrix}_{T,T} \\
&
\begin{pmatrix}
\bm{F}\bm{r}^{1\mathrm{h}+1\mathrm{p}} + \bm{V}^{2\mathrm{h}1\mathrm{p}}\bm{r}^{2\mathrm{h}1\mathrm{p}} + \bm{V}^{2\mathrm{h}1\mathrm{p}}\bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} + \bm{V}^{2\mathrm{plh}}\bm{r}^{2\mathrm{p}1\mathrm{h}} + \bm{V}^{2\mathrm{plh}}\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} \bm{r}^{1\mathrm{h}+1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xx} \bm{r}^{2\mathrm{h}1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xd} \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} + \bm{0} \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} \bm{r}^{1\mathrm{h}+1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{dx} \bm{r}^{2\mathrm{h}1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{dd} \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} + \bm{0} \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} \bm{r}^{1\mathrm{h}+1\mathrm{p}} + \bm{0} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xx} \bm{r}^{2\mathrm{p}1\mathrm{h}} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xd} \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} \bm{r}^{1\mathrm{h}+1\mathrm{p}} + \bm{0} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{dx} \bm{r}^{2\mathrm{p}1\mathrm{h}} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{dd} \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}
\end{pmatrix}_{T,\bar{A}}\\ &= E
\begin{pmatrix}
\bm{r}^{1\mathrm{h}+1\mathrm{p}} \\
\bm{r}^{2\mathrm{h}1\mathrm{p}} \\
\bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} \\
\bm{r}^{2\mathrm{p}1\mathrm{h}} \\
\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}
\end{pmatrix}_{T,\bar{A}}
\end{align}
So notice that the dimensions inside the super-metric must be as indicated. The left hand side becomes
\begin{align}
    \begin{pmatrix}
\bm{F}\bm{r}^{1\mathrm{h}+1\mathrm{p}} + \bm{V}^{2\mathrm{h}1\mathrm{p}}\bm{r}^{2\mathrm{h}1\mathrm{p}} + \bm{V}^{2\mathrm{h}1\mathrm{p}}\bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} + \bm{V}^{2\mathrm{plh}}\bm{r}^{2\mathrm{p}1\mathrm{h}} + \bm{V}^{2\mathrm{plh}}\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} \bm{r}^{1\mathrm{h}+1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xx} \bm{r}^{2\mathrm{h}1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xd} \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} + \bm{0} \\
\bm{N}_{A,A}\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger}_{A,P} \bm{r}^{1\mathrm{h}+1\mathrm{p}}_{P,\bar{A}} + \bm{N}_{A,A}[\bm{\tilde{C}}^{2\mathrm{hlp}}]^{dx}_{A,A} \bm{r}^{2\mathrm{h}1\mathrm{p}}_{A,\bar{A}} + \bm{N}_{A,A}[\bm{\tilde{C}}^{2\mathrm{hlp}}]^{dd}_{A,A} \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}}_{A,\bar{A}} + \bm{0} \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} \bm{r}^{1\mathrm{h}+1\mathrm{p}} + \bm{0} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xx} \bm{r}^{2\mathrm{p}1\mathrm{h}} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xd} \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} \\
\bm{N}_{\bar{A}, \bar{A}}\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger}_{\bar{A},P} \bm{r}^{1\mathrm{h}+1\mathrm{p}}_{P,\bar{A}} + \bm{0} + \bm{N}_{\bar{A}, \bar{A}}[\bm{\tilde{C}}^{2\mathrm{plh}}]^{dx}_{\bar{A},\bar{A}} \bm{r}^{2\mathrm{p}1\mathrm{h}}_{\bar{A},\bar{A}} + \bm{N}_{\bar{A}, \bar{A}}[\bm{\tilde{C}}^{2\mathrm{plh}}]^{dd}_{\bar{A},\bar{A}} \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}_{\bar{A},\bar{A}}
\end{pmatrix}_{T,\bar{A}}
\end{align}
This is the point at which we encounter issues in the derivation. See the alternate derivation below for a more straightforward approach that highlights this.
\begin{equation}
\begin{pmatrix}
\bm{1} & 0 & 0 \\
0 & \bm{1} \oplus \bm{N} & 0 \\
0 & 0 & \bm{1} \oplus \bm{N}
\end{pmatrix}
\begin{pmatrix}
\bm{F} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{h1p}} & \bm{V}^{2\mathrm{plh}} & \bm{V}^{2\mathrm{plh}} \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  &  & & \\
\left(\bm{V}^{2\mathrm{h1p}}\right)^{\dagger} &  & \bm{C}^{2\mathrm{hlp}} & & \bm{0} \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} & & & & \\
\left(\bm{V}^{2\mathrm{plh}}\right)^{\dagger} &  & \bm{0} & & \bm{C}^{2\mathrm{plh}}
\end{pmatrix}
\begin{pmatrix}
\bm{R}^{1\mathrm{h}+1\mathrm{p}} \\
\bm{R}^{2\mathrm{h}1\mathrm{p}} \\
\bm{R}^{2\mathrm{p}1\mathrm{h}} \\
\end{pmatrix} = E
\begin{pmatrix}
\bm{R}^{1\mathrm{h}+1\mathrm{p}} \\
\bm{R}^{2\mathrm{h}1\mathrm{p}} \\
\bm{R}^{2\mathrm{p}1\mathrm{h}} \\
\end{pmatrix}
\end{equation}
The 3 coupled equations can be written as
\begin{align}
\bm{F} \bm{R}^{1\mathrm{h}+1\mathrm{p}} +\bm{V}^{2\mathrm{h}1\mathrm{p}} \bm{R}^{2\mathrm{h}1\mathrm{p}}  + \bm{V}^{2\mathrm{plh}} \bm{R}^{2\mathrm{p}1\mathrm{h}} &= E \bm{R}^{1\mathrm{h}+1\mathrm{p}} \\
 \left(1\oplus N\right)\left(\bm{V}^{2\mathrm{h}1\mathrm{p}}\right)^{\dagger} \bm{R}^{1\mathrm{h}+1\mathrm{p}} + \left(1\oplus N\right) \left(\varepsilon^{1\mathrm{~h}} \oplus (-\bm{\tilde{M}})\right)\bm{R}^{2\mathrm{h}1\mathrm{p}} &= E \bm{R}^{2\mathrm{h}1\mathrm{p}} \\
\left(1\oplus N\right)\left(\bm{V}^{2\mathrm{p}1\mathrm{h}}\right)^\dagger \bm{R}^{1\mathrm{h}+1\mathrm{p}} + \left(1\oplus N\right)\left(\varepsilon^{1\mathrm{p}} \oplus \bm{\tilde{M}}\right)\bm{\tilde{C}}^{2\mathrm{p}1\mathrm{h}}\bm{R}^{2\mathrm{p}1\mathrm{h}} &= E \bm{R}^{2\mathrm{p}1\mathrm{h}}
\end{align}
and we now define $\bm{\tilde{C}}^{2\mathrm{hlp}} = \left(1\oplus N\right) \left(\varepsilon^{1\mathrm{~h}} \oplus (-\bm{\tilde{M}})\right) = \varepsilon^{1\mathrm{~h}} \oplus (-\bm{N}\bm{\tilde{M}})$ and $\bm{\tilde{C}}^{2\mathrm{plh}} = \left(1\oplus N\right)\left(\varepsilon^{1\mathrm{p}} \oplus \bm{\tilde{M}}\right) = \varepsilon^{1\mathrm{p}} \oplus \bm{N}\bm{\tilde{M}}$.
Now solve the last two equations for $\bm{R}^{2\mathrm{h}1\mathrm{p}}$ and $\bm{\bar{R}}^{2\mathrm{p}1\mathrm{h}}$:
\begin{align}
\bm{R}^{2\mathrm{h}1\mathrm{p}} &= \left(\bm{1}E - \bm{\tilde{C}}^{2\mathrm{hlp}}\right)^{-1} \left( \left(1\oplus N\right)\left(\bm{V}^{2\mathrm{h}1\mathrm{p}}\right)^\dagger \bm{R}^{1\mathrm{h}+1\mathrm{p}} \right)\\
\bm{\bar{R}}^{2\mathrm{p}1\mathrm{h}} &= \left(\bm{1}E - \bm{\tilde{C}}^{2\mathrm{plh}}\right)^{-1} \left( \left(1\oplus N\right)\left(\bm{V}^{2\mathrm{plh}}\right)^\dagger \bm{R}^{1\mathrm{h}+1\mathrm{p}} \right)
\end{align}
Substituting these into the first equation gives us
\begin{align}
\bm{F} \bm{R}^{1\mathrm{h}+1\mathrm{p}} &+ \bm{V}^{2\mathrm{h}1\mathrm{p}} \left(\bm{1}E - \bm{\tilde{C}}^{2\mathrm{hlp}}\right)^{-1} \left( \left(1\oplus N\right)\left(\bm{V}^{2\mathrm{h}1\mathrm{p}}\right)^\dagger \bm{R}^{1\mathrm{h}+1\mathrm{p}} \right)\\
&+  \bm{V}^{2\mathrm{plh}} \left(\bm{1}E - \bm{\tilde{C}}^{2\mathrm{plh}}\right)^{-1} \left( \left(1\oplus N\right)\left(\bm{V}^{2\mathrm{plh}}\right)^\dagger \bm{R}^{1\mathrm{h}+1\mathrm{p}} \right) = E \bm{R}^{1\mathrm{h}+1\mathrm{p}}
\end{align}
This gives us a form for the frequency dependent correlation self energy as
\begin{align}
\bm{\Sigma }^c(\omega ) &= \bm{V}^{2\mathrm{h}1\mathrm{p}} \left(\bm{1}\omega  - \bm{\tilde{C}}^{2\mathrm{hlp}}\right)^{-1} \left(1\oplus N\right) \left(\bm{V}^{2\mathrm{h}1\mathrm{p}}\right)^\dagger +  \bm{V}^{2\mathrm{plh}} \left(\bm{1}\omega  - \bm{\tilde{C}}^{2\mathrm{plh}}\right)^{-1} \left(1\oplus N\right)\left(\bm{V}^{2\mathrm{plh}}\right)^\dagger
\end{align}
 The next thing we need to do is to write the spectral decomposition of the $\bm{\tilde{C}}$ where for the 2h1p sector, this is $\bm{\tilde{C}}^{2\mathrm{hlp}} = \bm{U} \left(\bm{1}\left(\epsilon_k - \Omega_{\nu}\right)\right) \bm{U}^{-1} \implies \left( \bm{1}\omega  - \bm{\tilde{C}}^{2\mathrm{hlp}}\right)^{-1} = \bm{U} \left(\bm{1}\left[ \omega - \left(\epsilon_k - \Omega_\nu\right)\right]\right)^{-1} \bm{U}^{-1}$ and $\bm{\tilde{C}}^{2\mathrm{plh}} = \bm{U} \left(\bm{1}\left(\epsilon_k + \Omega_{\nu}\right)\right) \bm{U}^{-1} \implies \left( \bm{1}\omega  - \bm{\tilde{C}}^{2\mathrm{plh}}\right)^{-1} = \bm{U} \left(\bm{1}\left[ \omega  - \left(\epsilon_k + \Omega_\nu\right)\right]\right)^{-1} \bm{U}^{-1}$. Due to the bioorthonormality of the problem, the columns of $\bm{U}$ are the left eigenvectors of the $\bm{\tilde{C}}$ matrices, while the rows are the right eigenvectors. Thus, can we write
% \begin{align}
% \bm{\Sigma }^c(\omega ) &= 4 \frac{\bm{M}^{2\mathrm{h}1\mathrm{p}}\bm{M}^{2\mathrm{h}1\mathrm{p}}}{\omega  - \epsilon_k - \Omega_\nu} + 4 \frac{\bm{M}^{2\mathrm{plh}}\bm{M}^{2\mathrm{plh}}}{\omega  - \epsilon_k + \Omega_\nu}
% \end{align}
% where we have defined the transition densities $\bm{M}$ as a contraction between the RPA eigenvectors and the $\bm{V}$ matrix. 
\begin{align}
\bm\Sigma^c_{pq}(\omega)&=\sum_\nu \biggl[
    \sum_k\frac{\left(\sum_{[ia]}\bm{V^{2h1p}}_{p,k[ia]}|v_{[ia],\nu}^{R,2h1p}\rangle\right) \left(\sum_{[jb]} \langle v_{[jb],\nu}^{L,2h1p}|\,(\bm{V^{2h1p}}_{q,k[jb]})^\dagger\right)}
         {\omega - (\epsilon_k - \Omega_\nu)}\\
  &+ \sum_c\frac{\left(\sum_{[ia]}\bm{V^{2plh}}_{p,[ia]c}|v_{[ia],\nu}^{R,2plh}\rangle\right) \left(\sum_{[jb]} \langle v_{[jb],\nu}^{L,2plh}|\,(\bm{V^{2plh}}_{q,[jb]c})^\dagger\right)}
         {\omega - (\epsilon_c + \Omega_\nu)} \biggr]?
\end{align}
% \begin{align}
% \left(\bm(){V}^{2\mathrm{plh}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xx}\bm{r}^{2\mathrm{p}1\mathrm{h}}+ [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xd}\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} &= E \bm{r}^{2\mathrm{p}1\mathrm{h}} \\
% \left(\bm{V}^{2\mathrm{plh}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{dx}\bm{r}^{2\mathrm{p}1\mathrm{h}}+ [\bm{\tilde{C}}^{2\mathrm{plh}}]^{dd}\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} &= E \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}
% \end{align}
% If we make the definitions $\bm{R}_{\Sigma }^{2\mathrm{h}1\mathrm{p}} = \bm{r}^{2\mathrm{h}1\mathrm{p}} + \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}}$ and $\bm{R}_{\Sigma }^{2\mathrm{p}1\mathrm{h}} = \bm{r}^{2\mathrm{p}1\mathrm{h}} + \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}$, we can rewrite:
% \begin{align}
% \bm{F} \bm{r}^{1\mathrm{h}+1\mathrm{p}} + \bm{V}^{2\mathrm{h}1\mathrm{p}} \bm{R}_{\Sigma }^{2\mathrm{h}1\mathrm{p}} + \bm{V}^{2\mathrm{plh}} \bm{R}_{\Sigma }^{2\mathrm{p}1\mathrm{h}} &= E \bm{r}^{1\mathrm{h}+1\mathrm{p}} \\
% \left(\bm{V}^{2\mathrm{h}1\mathrm{p}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} + \left([\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xx} + [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xd} \right)\bm{R}_{\Sigma }^{2\mathrm{h}1\mathrm{p}} &= E \bm{r}^{2\mathrm{h}1\mathrm{p}} \\
% \left(\bm{V}^{2\mathrm{h}1\mathrm{p}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} - [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xd}\bm{r}^{2\mathrm{h}1\mathrm{p}} - [\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xx}\bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} &= E \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} \\
% \left(\bm{V}^{2\mathrm{plh}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} + [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xx}\bm{r}^{2\mathrm{p}1\mathrm{h}}+ [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xd}\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} &= E \bm{r}^{2\mathrm{p}1\mathrm{h}} \\
% \left(\bm{V}^{2\mathrm{plh}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} - [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xd}\bm{r}^{2\mathrm{p}1\mathrm{h}} - [\bm{\tilde{C}}^{2\mathrm{plh}}]^{xx}\bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} &= E \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}
% \end{align}
% We can combine into 3 by adding the second to the third and the fourth to the fifth, which gives us
% \begin{align}
% \bm{F} \bm{r}^{1\mathrm{h}+1\mathrm{p}} + \bm{V}^{2\mathrm{h}1\mathrm{p}} \left(\bm{r}^{2\mathrm{h}1\mathrm{p}} + \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}}\right) + \bm{V}^{2\mathrm{plh}} \left(\bm{r}^{2\mathrm{p}1\mathrm{h}} + \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}\right) &= E \bm{r}^{1\mathrm{h}+1\mathrm{p}} \\[6pt]
% 2\left(\bm{V}^{2\mathrm{h}1\mathrm{p}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} + \left([\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xx}+[\bm{\tilde{C}}^{2\mathrm{hlp}}]^{xd}\right)\bm{r}^{2\mathrm{h}1\mathrm{p}}+ \left([\bm{\tilde{C}}^{2\mathrm{hlp}}]^{dx}+[\bm{\tilde{C}}^{2\mathrm{hlp}}]^{dd}\right)\bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} &= E \bm{r}^{2\mathrm{h}1\mathrm{p}} \\[6pt]
% \end{align}

% Solving the last two equations gives:
% \begin{align}
% \bm{r}^{2\mathrm{h}1\mathrm{p}} + \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}} &= \left(E\bm{1} - \bm{\tilde{C}}^{2\mathrm{hlp}}\right)^{-1} \left(\left(\bm{V}^{2\mathrm{h}1\mathrm{p}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} - \bm{\tilde{C}}^{2\mathrm{plh}} \left(\bm{r}^{2\mathrm{p}1\mathrm{h}} + \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}}\right)\right) \\[6pt]
% \bm{r}^{2\mathrm{p}1\mathrm{h}} + \bm{\bar{r}}^{2\mathrm{p}1\mathrm{h}} &= \left(\bm{\tilde{C}}^{2\mathrm{plh}} - E \bm{1}\right)^{-1} \left(\left(\bm{V}^{2\mathrm{plh}}\right)^\dagger \bm{r}^{1\mathrm{h}+1\mathrm{p}} + \bm{\tilde{C}}^{2\mathrm{hlp}} \left(\bm{r}^{2\mathrm{h}1\mathrm{p}} + \bm{\bar{r}}^{2\mathrm{h}1\mathrm{p}}\right)\right)
% \end{align}
\subsubsection{Deriving matrix vector products for 2h1p sector}
The eigenvalue equation is then given by $\bm{H} \bm{R} = \bm{\mathcal{N}} \bm{R} \bm{E}$ and then leading to the non-Hermitian eigenvalue equation $\mathcal{\bm{N}} \bm{H} \bm{R} = \bm{R} \bm{E}$.
Let us carry this matrix multiplication out, just for the 2h1p sector. Collect the relevant components of the excitation vector into a block vector:
\[
\bm{R}_{T,A} = 
\begin{pmatrix}
\bm{r}_i &\bm{r}_a &
\bm{r}_{i[jb]} &
\bm{\bar{r}}_{i[jb]}
\end{pmatrix}^\dag
\]
We will define the total dimension of this excitation vector as $T=P+2A$, where $P=O+V$ and $A=O^2V$.
The super-Hamiltonian decomposes as:
\[
\bm{H} =
\begin{pmatrix}
\begin{pmatrix}
    \bm{F}_{oo} & \bm{F}_{ov} \\
    \bm{F}_{vo} & \bm{F}_{vv}
\end{pmatrix}_{P,P} &
\begin{pmatrix}
    \bm{V}^{2 \mathrm{h1p}}_{p,i[jb]} & \bm{V}^{2 \mathrm{h1p}}_{p,i[\bar{jb}]} \\
\end{pmatrix}_{P,2A} \\ 
\begin{pmatrix}
    \bm{V}^{2 \mathrm{h1p}}_{p,k[lc]} & \bm{V}^{2 \mathrm{h1p}}_{p,k[\bar{lc}]}
\end{pmatrix}^\dagger_{2A,P} &
\left(\left(\varepsilon^{1h}\right) \oplus_{\text{direct}} -\bm{\tilde{M}}_{jb,jb;lc,lc}\right)_{2A,2A}
\end{pmatrix}_{T,T}
\] 
The matrix-vector product becomes:
\[
\bm{\sigma}_{T,A}
= 
\bm{\mathcal{N}}_{T,T}
\begin{pmatrix}
 \begin{pmatrix}
\bm{F}_{oo} & \bm{F}_{ov} \\ \bm{F}_{vo} & \bm{F}_{vv}
\end{pmatrix}_{P,P} \begin{pmatrix}
\bm{r}_i \\ \bm{r}_a
\end{pmatrix}_{P,A} + \begin{pmatrix}
    \bm{V}^{2 \mathrm{h1p}}_{p,i[jb]} & \bm{V}^{2 \mathrm{h1p}}_{p,i[\bar{jb}]} \\
\end{pmatrix}_{P,2A} \begin{pmatrix}
    \bm{r}_{i[jb]} \\
\bm{\bar{r}}_{i[\bar{jb}]}
\end{pmatrix}_{2A,A} \\[6pt]
 \begin{pmatrix}
    \bm{V}^{2 \mathrm{h1p}}_{p,i[jb]} & \bm{V}^{2 \mathrm{h1p}}_{p,i[\bar{jb}]} \\
\end{pmatrix}^\dag_{2A,P} \begin{pmatrix}
\bm{r}_i \\ \bm{r}_a
\end{pmatrix}_{P,A} +
\left(\left(\varepsilon^{1h}\right) \oplus_{\text{direct}} -\bm{\tilde{M}}_{jb,jb;lc,lc}\right)_{2A,2A} \begin{pmatrix}
    \bm{r}_{i[jb]} \\
\bm{\bar{r}}_{i[\bar{jb}]}
\end{pmatrix}_{2A,A}
\end{pmatrix}
\]
Now we are left to decide what the super-metric $\bm{\mathcal{N}}$ is. We can define it as
\[\bm{\mathcal{N}} =
\begin{pmatrix}
\bm{I_{PP}} & 0 \\[3pt]
0 & \bm{N}_{2A,2A}
\end{pmatrix}_{T,T}
\]
 with $\bm{N}_{2A,2A} \equiv \bm{1}_{O} \oplus_{\text{direct}} \bm{N}_{\text{2OV}}$. 

% But if we take this definiton, then $\bm{\mathcal{N}}\bm{\mathcal{N}} \neq \bm{1}$, so we can't do $\bm{H} \bm{R} = \bm{\mathcal{N}} \bm{R} \bm{E} \implies \bm{\mathcal{N}} \bm{H} \bm{R} = \bm{R} \bm{E}$, as they claim. Next, assume they made a typo, so do $\bm{N}_{AA} = \begin{pmatrix}
%  \bm{1}_{O^2V} & 0 \\
% 0 & \bm{-1}_{O^2V}
%  \end{pmatrix}$. Now, we have  $\bm{N}_{AA}\bm{N}_{AA} = \bm{1}_{AA}$ and
\begin{align}
&    \bm{\mathcal{N}} \bm{H} \bm{R}\\
&= 
\begin{pmatrix}
\bm{1}_{P,P}
 \begin{pmatrix}
\bm{F}_{oo} & \bm{F}_{ov} \\ \bm{F}_{vo} & \bm{F}_{vv}
\end{pmatrix}_{P,P} \begin{pmatrix}
\bm{r}_i \\ \bm{r}_a
\end{pmatrix}_{P,A} + \bm{1}_{P,P}\begin{pmatrix}
    \bm{V}^{2 \mathrm{h1p}}_{p,i[jb]} & \bm{V}^{2 \mathrm{h1p}}_{p,i[{jb}]} \\
\end{pmatrix}_{P,2A} \begin{pmatrix}
    \bm{r}_{i[jb]} \\
\bm{\bar{r}}_{i[\bar{jb}]}
\end{pmatrix}_{2A,A} \\[6pt]
 \bm{N}_{2A,2A}\begin{pmatrix}
    \left(\bm{V}^{2 \mathrm{h1p}}_{p,i[jb]}\right)^\dag \\ \left(\bm{V}^{2 \mathrm{h1p}}_{p,i[{jb}]}\right)^\dag \\
\end{pmatrix}_{2A,P} \begin{pmatrix}
\bm{r}_i \\ \bm{r}_a
\end{pmatrix}_{P,A} +
\bm{N}_{2A,2A}\left(\left(\varepsilon^{1h}\right) \oplus_{\text{direct}} -\bm{\tilde{M}}_{jb,jb;lc,lc}\right)_{2A,2A} \begin{pmatrix}
    \bm{r}_{i[jb]} \\
\bm{\bar{r}}_{i[\bar{jb}]}
\end{pmatrix}_{2A,A}
\end{pmatrix} \\
&= \begin{pmatrix}
 \begin{pmatrix}
\bm{F}_{oo}\bm{r}_i + \bm{F}_{ov}\bm{r}_a \\ \bm{F}_{vo}\bm{r}_i + \bm{F}_{vv}\bm{r}_a
\end{pmatrix}_{P,A} + \begin{pmatrix}
    \bm{V}^{2 \mathrm{h1p}}_{k,i[jb]}\bm{r}_{i[jb]} + \bm{V}^{2 \mathrm{h1p}}_{k,i[{jb}]}\bm{\bar{r}}_{i[\bar{jb}]}\\ \bm{V}^{2 \mathrm{h1p}}_{c,i[jb]}\bm{r}_{i[jb]} + \bm{V}^{2 \mathrm{h1p}}_{c,i[{jb}]}\bm{\bar{r}}_{i[{jb}]}
\end{pmatrix}_{P,A} \\[6pt]
 \bm{N}_{2A,2A}\begin{pmatrix}
    \bm{V}^{2 \mathrm{h1p}}_{k,i[jb]}\bm{r}_k+ \bm{V}^{2 \mathrm{h1p}}_{a,i[jb]}\bm{r}_a \\
\bm{V}^{2 \mathrm{h1p}}_{k,i[jb]}\bm{r}_k+ \bm{V}^{2 \mathrm{h1p}}_{a,i[jb]}\bm{r}_a \\
\end{pmatrix}_{2A,A} +
\left(\left(\varepsilon^{1h}\right) \oplus_{\text{direct}} -\begin{pmatrix}
\bm{\tilde{M}}^{xx}_{jb,lc} & \bm{\tilde{M}}^{xd}_{jb,lc} \\
-\bm{\tilde{M}}^{dx}_{jb,lc} & -\bm{\tilde{M}}^{dd}_{jb,lc}
\end{pmatrix}\right)_{2A,2A} \begin{pmatrix}
    \bm{r}_{i[jb]} \\
\bm{\bar{r}}_{i[\bar{jb}]}
\end{pmatrix}_{2A,A}
\end{pmatrix} \\
% &= \begin{pmatrix}
%  \begin{pmatrix}
% \sum_j f_{ij}r_j+ \sum_b f_{ib}r_b + \sum_{jlc} \langle i c | jl \rangle r_{j[lc]} + \sum_{jlc} \langle i c | jl \rangle \bar{r}_{j[lc]} \\ \sum_j f_{aj}r_j+ \sum_b f_{ab}r_b + \sum_{jlc} \langle a c | jl \rangle r_{j[lc]} + \sum_{jlc} \langle a c | jl \rangle \bar{r}_{j[lc]}
% \end{pmatrix}_{P,A} \\[6pt]
% \begin{pmatrix}
%     \sum_k \langle ka | ij  \rangle {r}_k + \sum_b \langle ba | ij  \rangle {r}_b \\
% -\sum_k \langle ka | ij  \rangle {r}_k - \sum_b \langle ba | ij  \rangle {r}_b
% \end{pmatrix}_{2A,A} +
% \bm{N}_{AA}\left(\left(\varepsilon^{1h}\right) \oplus_{\text{kron}} -\begin{pmatrix}
% \bm{\tilde{M}}^{xx}_{jb,lc} & \bm{\tilde{M}}^{xd}_{jb,lc} \\
% \bm{\tilde{M}}^{dx}_{jb,lc} & \bm{\tilde{M}}^{dd}_{jb,lc}
% \end{pmatrix}\right)_{AA} \begin{pmatrix}
%     \bm{r}_{i[jb]} \\
% \bm{\bar{r}}_{i[\bar{jb}]}
% \end{pmatrix}_{2A,A}
% \end{pmatrix} \\
&= \begin{pmatrix}
 \begin{pmatrix}
\sum_j f_{ij}r_j+ \sum_b f_{ib}r_b + \sum_{jlc} \langle i c | jl \rangle r_{j[lc]} + \sum_{jlc} \langle i c | jl \rangle \bar{r}_{j[lc]} \\ \sum_j f_{aj}r_j+ \sum_b f_{ab}r_b + \sum_{jlc} \langle a c | jl \rangle r_{j[lc]} + \sum_{jlc} \langle a c | jl \rangle \bar{r}_{j[lc]}
\end{pmatrix}_{P,A} \\[6pt]
\begin{pmatrix}
    ?\\
 ?
\end{pmatrix}_{2A,A} +
 \begin{pmatrix}
? \\
?
\end{pmatrix}_{2A,A}
\end{pmatrix} \\
\end{align}
% \subsubsection{Proving equivalence to Booth's ED}
% Again, we will just work with a single channel, the lesser one. In the dRPA case, Booth's formulation for the upfolded Hamiltonian is
% \begin{equation}
%     \bm{H} = \begin{pmatrix} \bm{F} & \bm{W} \\ \bm{W}^{\dagger} & \bm{d} \end{pmatrix}
% \label{eq:booth_upfolded_hamiltonian}
% \end{equation}
% where we have the definitions
% \begin{equation}
%     W_{pkv} = \sum_{ia} (pk|ia) \left( X_{ia}^{v} + Y_{ia}^{v} \right) \quad \text{and} \quad d_{kv,lv'} = \left(\epsilon_k - \Omega_v\right) \delta_{k,l} \delta_{v,v'}
% \end{equation}
% Now Tim's version of the Hamiltonian for the lesser channel is given by
% \begin{equation}
%     \bm{H} = \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}} & \bm{V}^{2 \mathrm{h1p}} \\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & &\\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} && \bm{C}^{2 \mathrm{hlp}} \end{pmatrix}
% \label{eq:tim_upfolded_hamiltonian}
% \end{equation}
% where the formal defintions are $\bm{C}^{2 \mathrm{hlp}} = \epsilon^{1 \mathrm{~h}} \oplus (-\tilde{\bm{M}})$ and $\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}} = \langle p c | k l \rangle $. Note that now the auxiliary block contain two seperate paricle-hole excitations, which is why the coupling blocks are duplicated. Let us define a unitary rotation $\bm{U} = \bm{1} \otimes \bm{Z}$. Application of this unitary to the Hamiltonian will not change the spectrum and actually transforms the problem into
% \begin{equation}
%     \bm{H}' = \bm{U}^\dag \bm{H} \bm{U} = \begin{pmatrix} \bm{1} & \bm{0}\\ \bm{0}&\bm{Z }^\dag \end{pmatrix} \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}} & \bm{V}^{2 \mathrm{h1p}} \\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & &\\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} && \bm{C}^{2 \mathrm{hlp}} \end{pmatrix} \begin{pmatrix} \bm{1} & \bm{0} \\ \bm{0}&\bm{Z } \end{pmatrix}
% = \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}}\bm{Z} & \bm{V}^{2 \mathrm{h1p}}\bm{Z} \\ \bm{Z}^\dag \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & &\\ \bm{Z}^\dag \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} && \bm{Z }^\dagger \bm{C}^{2 \mathrm{hlp}} \bm{Z } \end{pmatrix}.
% \label{eq:tim_upfolded_hamiltonian}
% \end{equation}
\subsection{Proving ED equivalence to Booth for the dRPA using Z-vector trick}
The super matrix that downfolds into the GW self-energy is
\begin{align}
\begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}} & \bm{V}^{2 \mathrm{p} 1 \mathrm{~h}} \\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \bm{C}^{2 \mathrm{hlp}} & \bm{0} \\ \left(\bm{V}^{2 \mathrm{plh}}\right)^{\dagger} & \bm{0} & \bm{C}^{2 \mathrm{plh}} \end{pmatrix}
\end{align}
For dRPA, we will choose 
\begin{equation}
    \bm{C}^{2 \mathrm{hlp}} = \epsilon^{1 \mathrm{~h}} \oplus (-\bm{M}^{1/2})
\implies C_{i[j a], k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} = \left[\epsilon _i - \bm{M^{1/2}}_{ja,lc}\right] \delta_{i k}
\end{equation}
 where $\bm{M}$ is the matrix defined in \eqref{eq:berk_gw_M}.
If we choose our unitary transformation to be $\bm{U'} = \bm{1} \oplus \bm{T}$, where $\bm{T}$ is the matrix of eigenvectors of the symmetric formulation as 
\begin{equation}
    \bm{M}\bm{T}= \bm{\lambda } \bm{T}
\label{eq:berk_gw_eigenproblem}
\end{equation}
with the matrix $\bm{M}$ defined as
\begin{equation}
    \bm{M}=(\bm{A}-\bm{B})^{1 / 2}(\bm{A}+\bm{B})(\bm{A}-\bm{B})^{1 / 2}
\label{eq:berk_gw_M}
\end{equation}
with the identities $\bm{\lambda } = \bm{\Omega }^2$ and $\bm{T} = \bm{\Omega }^{1/2} \left[\bm{A}-\bm{B}\right]^{-1/2}\left(\bm{X} + \bm{Y}\right)$, where $\bm{X}$ and $\bm{Y}$ are the excitation and de-excitation vectors, respectively. 
\subsubsection{Casida transformation}

The matrix problem is:
\begin{equation}
\begin{bmatrix}
\textbf{A} & \textbf{B} \\
-\textbf{B} & -\textbf{A}
\end{bmatrix}
\begin{bmatrix}
\textbf{X}&\textbf{Y} \\
\textbf{Y} &\textbf{X}
\end{bmatrix}
= 
\begin{bmatrix}
\textbf{X} & \textbf{Y}\\
\textbf{Y} & \textbf{X}
\end{bmatrix}
\begin{bmatrix}
\boldsymbol{\Omega } & 0 \\
0 & -\boldsymbol{\Omega }
\end{bmatrix}
.
\end{equation}
which becomes equivalent to the pair of linear equations for each excitation mode
\begin{equation}
\begin{split}
    \left(\textbf{A} + \textbf{B}\right) \left(X + {Y}\right)_\nu = \Omega_\nu \left({X} - {Y}\right)_\nu \\
    \left(\textbf{A} - \textbf{B}\right) \left(X - {Y}\right)_\nu = \Omega_\nu \left(X + {Y}\right)_\nu
\label{eqn:A_B}
\end{split}
\end{equation}
We know we have the symmetric eigenproblem $\bm{M}T_\nu = \lambda_\nu T_\nu$, where $\bm{M}=(\bm{A}-\bm{B})^{1/2}(\bm{A}+\bm{B})(\bm{A}-\bm{B})^{1/2}$. Let us make the ansatze 
\begin{equation}
\begin{split}
    \left({X} + {Y}\right)_\nu &= {\Omega }_\nu^{-\frac{1}{2}} \left(\textbf{A}-\textbf{B}\right)^{\frac{1}{2}}T_\nu \\
    \left({X} - {Y}\right)_\nu &= {\Omega }_\nu^{\frac{1}{2}} \left(\textbf{A}-\textbf{B}\right)^{-\frac{1}{2}}T_\nu
\label{eqn:XY_ansatz}
\end{split}
\end{equation}
We know that this satisfies the biorthogonality relation since
\begin{equation}
    \left({X} + {Y}\right)_\nu^\dagger \left({X} - {Y}\right)_\nu = \left({\Omega }_\nu^{-\frac{1}{2}} \left(\textbf{A}-\textbf{B}\right)^{\frac{1}{2}}T_\nu\right)^\dagger \left({\Omega }_\nu^{\frac{1}{2}} \left(\textbf{A}-\textbf{B}\right)^{-\frac{1}{2}}T_\nu\right) = T_\nu^\dagger T_\nu = {1}
\end{equation}
Now, we can plug the ansatz \ref{eqn:XY_ansatz} into the equations \ref{eqn:A_B} to get
\begin{equation}
\begin{split}
    \left(\textbf{A} + \textbf{B}\right) \left({\Omega }_\nu^{-\frac{1}{2}} \left(\textbf{A}-\textbf{B}\right)^{\frac{1}{2}}T_\nu\right) &= {\Omega }_\nu \left( {\Omega }_\nu^{\frac{1}{2}} \left(\textbf{A}-\textbf{B}\right)^{-\frac{1}{2}}T_\nu\right) \\
    \implies {\Omega }_\nu^{-\frac{1}{2}} \left(\textbf{A} + \textbf{B}\right) \left(\textbf{A}-\textbf{B}\right)^{\frac{1}{2}}T_\nu &= {\Omega }_\nu^{\frac{3}{2}} \left(\textbf{A}-\textbf{B}\right)^{-\frac{1}{2}}T_\nu \\
\implies \left(\textbf{A}-\textbf{B}\right)^{\frac{1}{2}} \left(\textbf{A} + \textbf{B}\right) \left(\textbf{A}-\textbf{B}\right)^{\frac{1}{2}}T_\nu &= {\Omega }_\nu^{2} T_\nu
\end{split}
\end{equation}
We make the ansatz that the eigenvectors of the symmetric formulation are given by $\textbf{T} = \boldsymbol{\Omega }^{\frac{1}{2}} \left(\textbf{A}-\textbf{B}\right)^{-\frac{1}{2}}\left(\textbf{X} + \textbf{Y}\right)$. Plugging into \ref{eqn:Aplus_B} gives us
\begin{align}
    \left(\textbf{A} + \textbf{B}\right) \left(\boldsymbol{\Omega }^{-\frac{1}{2}} \left(\textbf{A}-\textbf{B}\right)^{\frac{1}{2}}\bm{T}\right) &= \boldsymbol{\Omega } \left( \bm{X} - \bm{Y}\right) \\
\bm{\Omega}^{-\frac{1}{2}} \left(\textbf{A} + \textbf{B}\right) \left(\textbf{A}-\textbf{B}\right)^{\frac{1}{2}}\bm{T} &= \boldsymbol{\Omega } \left( \bm{X} - \bm{Y}\right) \\
\bm{\Omega}^{-\frac{1}{2}}\left(\textbf{A}-\textbf{B}\right)^{-\frac{1}{2}} \bm{M} \bm{T} &= \boldsymbol{\Omega } \left( \bm{X} - \bm{Y}\right) \\
\end{align}
Then, we can apply the unitary transformation to the Hamiltonian in Tim's formulation as
\begin{equation}
    \bm{H'} = \bm{U'}^\dag \bm{H} \bm{U'} = \begin{pmatrix} \bm{1} & \bm{0}\\ \bm{0}&\bm{T }^\dag \end{pmatrix} \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}}\\ \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \bm{C}^{2 \mathrm{hlp}} \end{pmatrix} \begin{pmatrix} \bm{1} & \bm{0} \\ \bm{0}&\bm{T } \end{pmatrix} = \begin{pmatrix} \bm{F} & \bm{V}^{2 \mathrm{h1p}}\bm{T}\\ \bm{T}^\dag \left(\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \bm{T }^\dagger \bm{C}^{2 \mathrm{hlp}} \bm{T } \end{pmatrix}.
\label{eq:tim_upfolded_hamiltonian}
\end{equation}
But now let us try evaluating the term $\bm{T}^\dagger \bm{C}^{2 \mathrm{hlp}} \bm{T }$ in the dRPA case. We have
\begin{align}
    \bm{T}^\dagger \bm{C}^{2 \mathrm{hlp}} \bm{T } &= \bm{T}^\dagger \left(\epsilon^{1 \mathrm{~h}} \otimes \bm{1} + \bm{1} \otimes (-\bm{M}^{1/2})\right) \bm{T } \\
&= \bm{T}^\dagger \left(\epsilon^{1 \mathrm{~h}} \otimes \bm{1}\right) \bm{T } + \bm{T}^\dagger \left(\bm{1} \otimes (-\bm{M}^{1/2})\right) \bm{T } \\
&= \left(\epsilon^{1 \mathrm{~h}} \underbrace{\bm{T}^\dag \bm{T}}_{\bm{\bm{I}}} \right) \otimes \left(\underbrace{\bm{T}^\dag \bm{T}}_{\bm{\bm{I}}}\right) + \left(\underbrace{\bm{T}^\dag \bm{T}}_{\bm{\bm{I}}}\right) \otimes (-\underbrace{\bm{T}^\dag \bm{M}^{1/2} \bm{T}}_{\bm{\Omega }}) \\
\end{align}
Next, let us consider the product $\bm{V}^{2 \mathrm{~h} 1 \mathrm{p}} \bm{T }$. We have
\begin{align}
    \bm{V}^{2 \mathrm{~h} 1 \mathrm{p}} \bm{T } &= \sum_{ia} (pk|ia) \left( X_{ia}^{v} + Y_{ia}^{v} \right) \Omega_{ia;v}^{1/2} \underbrace{\sum_{jb} \left(A_{ia,jb} - B_{ia,jb}\right)^{-1/2}}_{\Delta _{ia}^{-1/2}} \\
\end{align}
This would only work if we could assume that $\left(\frac{\Omega _{ia;v}}{\Delta _{ia}}\right)^{1/2} \approx 1$. 
\subsection{Evaluating the matrix-vector products}
Now, we can define a vector $\bm{R} = ( r_i,\; r_a,\; r_{i[jb]},\; r_{[jb]a} )$. Application of the Hamiltonian to this vector gives us the matrix-vector product $\bm{H'} \bm{R} = \bm{\sigma }$, where $\bm{\sigma} = ( \sigma_i,\; \sigma_a,\; \sigma_{i[jb]},\; \sigma_{[jb]a} )$. So the one body matrix vector products are the same as in the dTDA case
\begin{align}
\sigma_i &= 
  \sum_{j} f_{i j}\,r_j
  + \sum_{b} f_{i b}\,r_b
  + \sum_{k l c} \bigl\langle i\,c | k\,l \bigr\rangle\,r_{k[l c]}
  + \sum_{k c d} \bigl\langle i\,k | d\,c \bigr\rangle\,r_{[k c]d}, \\[6pt]
\sigma_a &=
    \sum_{j} f_{a j}\,r_j
    + \sum_{b} f_{a b}\,r_b
    + \sum_{k l c} \bigl\langle a\,c | k\,l \bigr\rangle\,r_{k[l c]}
    + \sum_{k c d} \bigl\langle a\,k | d\,c \bigr\rangle\,r_{[k c]d}, \\[6pt]
\end{align}
For the two-body matrix vector products, we have
\begin{align}
\sigma_{i[ja]} &=
    \sum_{k} \bigl\langle k\,a | i\,j \bigr\rangle\,r_k
    + \sum_{b} \bigl\langle b\,a | i\,j \bigr\rangle\,r_b
    + \sum_{klc} C_{i[j a], k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} r_{i[l c]}
\end{align}
Now, consider just
\begin{equation}
    \sum_{klc} C_{i[j a], k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} r_{i[l c]}= \sum_{klc} \left[\left(\epsilon_i-\bm{M}^{1/2}_{ja,lc}\right) \delta_{ik} r_{i[l c]} \right] = \sum_{l c} \left(\epsilon_i - \bm{M}^{1/2}_{ja,lc}\right) r_{i[l c]}
\end{equation}
Consider now just the term 
\begin{equation}
    \sum_{l c} \bm{M}^{1/2}_{ja,lc} r_{i[l c]} =  \sum_{l c} \left[\underbrace{\Delta _{ja}^{1/2} \left(\Delta_{ja} + 2(ja|lc)\right)_{ja,lc} \Delta_{ja}^{1/2}}_{\bm{D}}\right]^{1/2} r_{i[l c]}.
\end{equation}
Let us consider instead the similarity transformed $\tilde{\bm{D}} = \Delta^{-1/2} \bm{D} \Delta^{-1/2} = \left(\Delta + 2K\right)$. This is useful because we have $D^{1/2} = \Delta^{1/2}\tilde{D}^{1/2}\Delta^{1/2}=\left(\Delta+2K\right)^{1/2}$ for our SPD D.Now make the change of variables $u = \Delta^{1/2}r \implies r = \Delta^{-1/2}u$. Then we have $\left(\Delta + 2K\right) \Delta^{-1/2}u = \Delta^{1/2} \left[ {I} + \underbrace{2\Delta^{-1/2}K\Delta^{-1/2}}_{L}\right] u$. Again, because we are working with weak to moderately correlated systems, we can assume that L is small, and neglect it for now. So we have
\begin{equation}
    \sum_{l c} \bm{M}^{1/2}_{ja,lc} r_{i[l c]} \approx \sum_{l c} \Delta^{1/2} I \Delta^{1/2} r_{i[l c]} \approx \sum_{l c} \Delta r_{i[l c]}
\end{equation}

We can rewrite this as
So really we have reduced the scaling to $O(O^3V^2)$. If we do a similar thing for the other term, we have
\begin{align}
\sigma_{[ia]b} &= 
    \sum_{j} \bigl\langle j\,i | b\,a \bigr\rangle\,r_j
    + \sum_{c} \bigl\langle c\,i | b\,a \bigr\rangle\,r_c
    + \sum_{kc} \left[\epsilon_b + \bm{M}^{1/2}_{ia,kc}\right] r_{[kc] b}
\end{align}
which gives the scaling of $O(O^2V^3)$.
\section{Extra}
Recall that we are trying to recover the form 
\begin{equation}
    \bm{H} = \begin{pmatrix} \bm{F} & \bm{W}^<\\ \bm{W}^{<,\dagger} & \bm{d}^< \end{pmatrix}
\label{eq:booth_upfolded_hamiltonian2}
\end{equation}
where now we have the definitions
\begin{align}
    W_{pkv}^{<} &= \sum_{ia} (pk|ia) \left( X_{ia}^{v} + Y_{ia}^{v} \right) \\
    d_{kv,lv'}^{<} &= \left(\epsilon_k - \Omega_v\right) \delta_{k,l} \delta_{v,v'}
\label{eq:booth_definitions2}
\end{align}
where now $\bm{\Omega }$ is taken fromm the eigenproblem $\bm{M}\bm{T}= \bm{\Omega}^2 \bm{T}$, which is the symmetric formulation of the RPA.
Application of downfolding on this upfolded form then gave us the expression
\begin{equation}
    \mathbf{\Sigma }(\omega )=\mathbf{W}^<(\omega \mathbf{I}-{\mathbf{d}^<})^{-1} {\mathbf{W}}^{<,\dagger} + \mathbf{W}^>(\omega \mathbf{I}-{\mathbf{d}^>})^{-1} {\mathbf{W}}^{>,\dagger}
\label{eq:booth_self_energy}
\end{equation}
Meanwhile, downfolding Tim's formulation gives us the expression for the correlation self energy
\begin{equation}
    \Sigma(\omega)= \mathbf{V}^{2 \mathrm{hlp}}\left[\omega \mathbf{1}-\mathbf{C}^{2 \mathrm{hlp}}\right]^{-1}\left[\mathbf{V}^{2 \mathrm{hlp}}\right]^{\dagger} +\mathbf{V}^{2 \mathrm{plh}}\left[\omega \mathbf{1}-\mathbf{C}^{2 \mathrm{plh}}\right]^{-1}\left[\mathbf{V}^{2 \mathrm{plh}}\right]^{\dagger}
\end{equation}
where in the dTDA case the supermatrix is
\begin{equation}
    \mathbf{H}=\left(\begin{array}{ccc}
\mathbf{f} & \mathbf{V}^{2 \mathrm{hlp}} & \mathbf{V}^{2 \mathrm{p} 1 \mathrm{~h}} \\
\left(\mathbf{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \mathbf{C}^{2 \mathrm{hlp}} & \mathbf{0} \\
\left(\mathbf{V}^{2 \mathrm{plh}}\right)^{\dagger} & \mathbf{0} & \mathbf{C}^{2 \mathrm{plh}}
\end{array}\right)
\end{equation}
with the definitions
\begin{align}
    V_{p, k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} &= \langle p c | k l \rangle \\
    V_{p,[k c] d}^{2 \mathrm{p} 1 \mathrm{~h}} &= \langle p k | d c \rangle \\
    C_{i[j a], k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} &= \left[\left(\epsilon_i+\epsilon_j-\epsilon_a\right) \delta_{j l} \delta_{a c}-\langle j c | a l \rangle\right] \delta_{i k} \\
    C_{[i a] b,[k c] d}^{2 \mathrm{p} 1 \mathrm{~h}} &= \left[\left(\epsilon_a+\epsilon_b-\epsilon_i\right) \delta_{i k} \delta_{a c}+\langle a k | i c\rangle\right] \delta_{b d}
\end{align}
Presumably they should be equivalent but I am a little bet confused. In both kisses we have physical and auxiliary spaces. However, Tim uses a bare Coulomb interaction for the coupling matrix, while Booth uses the screened interaction. Then also the definitions of the auxiliary spaces $\bm{d}$ versus $\bm{C}$ are different, where the former makes use of the $\Omega $, assuming that the RPA calculation has already been performed, while the latter is just defined by the A matrix in the dTDA case.  Please show how to get from Tim's formulation to Booth's formulation, or vice versa, and how the two are equivalent. let's just focus on a single channel, so just the lesser channel. Let us start by proving for the dTDA case, where we have the definitions for Booth's formulation
\begin{align}
    W_{pkv}^{<} &= \sum_{ia} (pk|ia) \left( X_{ia}^{v} \right) \\
    d_{kv,lv'}^{<} &= \left(\epsilon_k - \Omega_v\right) \delta_{k,l} \delta_{v,v'}
\end{align}
where in the dtda case, we are working with the eigenproblem $\bm{A}\bm{X} = \bm{\Omega }\bm{X}$.
while in Tim's formulation we have
\begin{align}
    \bm{C}^{2 \mathrm{hlp}} &= \epsilon^{1 \mathrm{~h}} \oplus (-\bm{A}) \\
\end{align}
with the metrics aliments defined as
\begin{align}
    V_{p, k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} &= \langle p c | k l \rangle \\
    C_{i[j a], k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}} &= \left[\left(\epsilon_i+\epsilon_j-\epsilon_a\right) \delta_{j l} \delta_{a c}-\langle j c | a l \rangle\right] \delta_{i k}
\end{align}
\section{Derivation with RPA screening using Furche's symmetric formulation}
\label{sec:berk_gw}
In the symmetric formulation, we have the hermitian eigenproblem:
\begin{equation}
    \bm{M}\bm{T}= \bm{\lambda } \bm{T}
\label{eq:berk_gw_eigenproblem}
\end{equation}
where we define
\begin{equation}
    \bm{M}=(\bm{A}-\bm{B})^{1 / 2}(\bm{A}+\bm{B})(\bm{A}-\bm{B})^{1 / 2}
\end{equation}
with the $\bm{A}$ and $\bm{B}$ matrices defined as
\begin{align}
    A_{i a, j b} &= \left(\epsilon_a-\epsilon_i\right) \delta_{i j} \delta_{a b}+\mathcal{K}_{i a, b j} \\
    B_{i a, j b} &= \mathcal{K}_{i a, j b}.
\end{align}
From this eigenproblem, we can get the quantities that we need for GW as
\begin{equation}
    \begin{aligned}
& \Omega_n=\sqrt{\lambda_n} \\
& (X+Y)_n=\frac{1}{\sqrt{\Omega_n}}(A-B)^{1 / 2} T_n \\
\end{aligned}
\end{equation}
In direct RPA, one defines a frequency-independent "super-matrix" H,
$$
\mathbf{H}=\left(\begin{array}{ccc}
\mathbf{f} & \mathbf{V}^{2 \mathrm{hlp}} & \mathbf{V}^{2 \mathrm{p} 1 \mathrm{~h}} \\
\left(\mathbf{V}^{2 \mathrm{~h} 1 \mathrm{p}}\right)^{\dagger} & \mathbf{C}^{2 \mathrm{hlp}} & \mathbf{0} \\
\left(\mathbf{V}^{2 \mathrm{plh}}\right)^{\dagger} & \mathbf{0} & \mathbf{C}^{2 \mathrm{plh}}
\end{array}\right)
$$
If the Tamm-Dancoff approximation is used, we have $\mathbf{C}^{2 \text { hlp }}=\varepsilon^{1 \mathrm{~h}} \oplus(-\mathbf{A})$ and $\mathbf{C}^{2 \mathrm{plh}}=\varepsilon^{1 \mathrm{p}} \oplus \mathbf{A}$. 

with matrix elements
$$
\begin{gathered}
V_{p, k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}}=\langle p c | k l\rangle \\
V_{p,[k c] d}^{2 \mathrm{p} 1 \mathrm{~h}}=\langle p k | d c\rangle \\
C_{i[j a], k[l c]}^{2 \mathrm{~h} 1 \mathrm{p}}=\left[\left(\varepsilon_i+\varepsilon_j-\varepsilon_a\right) \delta_{j l} \delta_{a c}-\langle j c | a l\rangle\right] \delta_{i k} \\
C_{[i a] b,[k c] d}^{2 \mathrm{p} 1 \mathrm{~h}}=\left[\left(\varepsilon_a+\varepsilon_b-\varepsilon_i\right) \delta_{i k} \delta_{a c}+\langle a k | i c\rangle\right] \delta_{b d}
\end{gathered}
$$
But now, to incorporate the RPA screening, we can instead define $\bm{C}^{2 \text{h1p}} \equiv \varepsilon^{1 \mathrm{~h}} \oplus(-\bm{M})$ and $\bm{C}^{2 \mathrm{p} 1 \mathrm{~h}} \equiv \varepsilon^{1 \mathrm{p}} \oplus \bm{M}$, where $\bm{M}$ is the matrix defined above. We can get the matrix elements of $\bm{M}$ as
\begin{align}
    M_{i a, j b} &= \left(\bm{A}-\bm{B}\right)^{1 / 2}_{i a, j b} \left(\bm{A}+\bm{B}\right)_{i a, j b} \left(\bm{A}-\bm{B}\right)^{1 / 2}_{i a, j b} \\
\\
&= \sqrt{\Delta _{ia}}\left[\left(\epsilon_a-\epsilon_i\right) \delta_{i j} \delta_{a b}+2 \mathcal{K}_{i a, b j}
\right] \sqrt{\Delta _{jb}}. \\
\end{align}
In the second line we have used the fact that in the direct formulation $\bm{A}-\bm{B}$ is purely diagonal, so in fact $\left(\bm{A}-\bm{B}\right)^{1/2}$ is just the square root of the diagonal elements, which we denote as $\sqrt{\Delta_{ia}}$. Furthermore, we have that the interaction kernels $\mathcal{K}_{ia,bj} \equiv \mathcal{K}_{i a, j b}$ for the HF reference, where in the physicist's notation we have $\mathcal{K}_{i a, j b} = \langle i j | a b \rangle$. So the expressions become
\begin{align}
    \bm{C}^{2 \text{h1p}} &= \epsilon^{1 \mathrm{~h}} \oplus (-\bm{M}) \\
& = \begin{pmatrix}
    \epsilon ^{1h} & 0 \\
    0 & -\bm{M}
\end{pmatrix}
\end{align}
and
\begin{align}
    \bm{C}^{2 \mathrm{p} 1 \mathrm{~h}} &= \epsilon^{1 \mathrm{p}} \oplus \bm{M} \\
& = \begin{pmatrix}
    \epsilon ^{1p} & 0 \\
    0 & \bm{M}
\end{pmatrix}
\end{align}
where $\epsilon^{1 \mathrm{~h}}$ and $\epsilon^{1 \mathrm{p}}$ are the diagonal matrices of the one-electron energies for the hole and particle states, respectively. 

% Now, we actually want to evaluate the metrics vector products, so we define a excitation vector $\boldsymbol{R} = ( r_i,\; r_a,\; r_{i[jb]},\; r_{[jb]a} )$ and the matrix-vector product $\bm{H} \boldsymbol{R} = \bm{\sigma }$, where $\bm{\sigma} = ( \sigma_i,\; \sigma_a,\; \sigma_{i[jb]},\; \sigma_{[jb]a} )$. Let us enumerate now what we actually will get:
% \begin{align}
% \sigma_i &= 
%   \sum_{j} f_{i j}\,r_j
%   + \sum_{b} f_{i b}\,r_b
%   + \sum_{k l c} \bigl\langle i\,c | k\,l \bigr\rangle\,r_{k[l c]}
%   + \sum_{k c d} \bigl\langle i\,k | d\,c \bigr\rangle\,r_{[k c]d}, \\[6pt]
% \sigma_a &= 
%   \sum_{j} f_{a j}\,r_j
%   + \sum_{b} f_{a b}\,r_b
%   + \sum_{j l c} \bigl\langle a\,c | j\,l \bigr\rangle\,r_{j[l c]}
%   + \sum_{k c d} \bigl\langle a\,k | d\,c \bigr\rangle\,r_{[k c]d}, \\[6pt]
% \sigma_{i[jb]} &
%   \sum_{k} \bigl\langle k\,b | i\,j \bigr\rangle\,r_k
%   + \sum_{a} \bigl\langle a\,j | b\,i \bigr\rangle\,r_a
%   + M_{i a,\,j b}\,r_{i[jb]}, \\[6pt]
% \sigma_{[jb]a} &= 
%   \sum_{k} \bigl\langle k\,j | a\,b \bigr\rangle\,r_k
%   + \sum_{c} \bigl\langle c\,b | j\,a \bigr\rangle\,r_c
%   + M_{i a,\,j b}\,r_{[jb]a}.
% \end{align}
% The interesting part will be to figure out how to evaluate elements like $M_{i a, j b}r_{i[jb]}$. We can write this as
% \begin{align}
%     M_{i a, j b} r_{i[jb]} &= \sqrt{\Delta_{ia}} \left[\left(\epsilon_a-\epsilon_i\right) \delta_{i j} \delta_{a b}+2 \mathcal{K}_{i a, b j}\right] \sqrt{\Delta_{jb}} r_{i[jb]} \\
% &= \sqrt{\Delta_{ia}} \left[\left(\epsilon_a-\epsilon_i\right) \delta_{i j} \delta_{a b}+2 \mathcal{K}_{i a, b j}\right] r_{i[jb]} \sqrt{\Delta_{jb}}.
% \end{align}